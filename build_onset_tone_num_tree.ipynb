{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'calculate_frequencies' from 'd:\\\\Research\\\\tonal-adaptation\\\\calculate_frequencies.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from dragonmapper import hanzi\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import set_config\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text, _tree\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, accuracy_score, classification_report, matthews_corrcoef\n",
    "\n",
    "import transform_data\n",
    "importlib.reload(transform_data)\n",
    "import calculate_frequencies\n",
    "importlib.reload(calculate_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = {}\n",
    "\n",
    "with open(\"./data/output_data.json\", \"r\") as output_data_file:\n",
    "    try:\n",
    "        data = json.load(output_data_file)[\"words\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "onset_frequencies = { }\n",
    "onset_zh_frequencies = { }\n",
    "tone_frequencies = { }\n",
    "\n",
    "for word_en, word_data in data.items():\n",
    "    onset = word_data[\"onset_en_ipa\"]\n",
    "    onset_frequencies[onset] = onset_frequencies[onset] + 1 if onset in onset_frequencies else 1\n",
    "\n",
    "    onset_zh = word_data[\"word_zh\"][0]\n",
    "    onset_zh_frequencies[onset_zh] = onset_zh_frequencies[onset_zh] + 1 if onset_zh in onset_zh_frequencies else 1\n",
    "\n",
    "    tone = word_data[\"onset_tone_num\"]\n",
    "    tone_frequencies[tone] = tone_frequencies[tone] + 1 if tone in tone_frequencies else 1\n",
    "\n",
    "frequencies, cond_probs = calculate_frequencies.get_data()\n",
    "\n",
    "onset_en_ipa_list = list(onset_frequencies.keys())\n",
    "onset_zh_list = list(onset_zh_frequencies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X_labels = []\n",
    "X_labels.append(\"Onset N(V)\")\n",
    "X_labels.append(\"Onset Length\")\n",
    "X_labels += [\"close\", \"near-close\", \"close-mid\", \"mid\", \"open-mid\", \"near-open\", \"open\"]\n",
    "X_labels += [\"front\", \"central\", \"back\"]\n",
    "X_labels.append(\"Onset N(C)\")\n",
    "X_labels.append(\"initial_stress_en\")\n",
    "X_labels.append(\"first_stress_en\")\n",
    "X_labels.append(f\"F(Onset)\")\n",
    "X_labels += [f\"P(Onset|T={tone})\" for tone in range(1,5)]\n",
    "X_labels += [f\"P(T={tone}|Onset)\" for tone in range(1,5)]\n",
    "X_labels += [f\"P(Onset|Hanzi={onset_zh})\" for onset_zh in onset_zh_list]\n",
    "X_labels += [f\"P(Hanzi={onset_zh}|Onset)\" for onset_zh in onset_zh_list]\n",
    "# X_labels += [\"N(Syllables)\"]\n",
    "# X_labels += [\"Onset%Word\"]\n",
    "X_labels += [\"Bisyllabic\"]\n",
    "\n",
    "y_labels = [\"1\", \"2\", \"3\", \"4\"]\n",
    "# y_labels = [\"Tone 1\", \"Tone 2\", \"Tone 3\", \"Tone 4\"]\n",
    "# y_labels = [\"Tone 1/2\", \"Tone 3\", \"Tone 4\"]\n",
    "# y_labels = [\"Tone 1/2\", \"Tone 3/4\"]\n",
    "# y_labels = [\"Tone 1\", \"Tone 2/3/4\"]\n",
    "# y_labels = [\"Tone 1/3\", \"Tone 2/4\"]\n",
    "\n",
    "confident_onset_zh_list = [\n",
    "    # \"伊\", \"安\", # 1\n",
    "    # \"艾\", # 2\n",
    "    # \"埃\", # 3\n",
    "    # \"阿\", # 4\n",
    "    # \"亚\", \"以\", \"奥\" # 5\n",
    "    ###\n",
    "    # \"艾\", # 1\n",
    "    # \"安\", # 1\n",
    "    # \"伊\", # 1\n",
    "    # \"亚\", # 2\n",
    "    # \"埃\", # 2\n",
    "    # \"奥\", # 2\n",
    "    # \"阿\", # 3\n",
    "    # \"以\", # 4\n",
    "    # \"爱\" # 5\n",
    "    ###\n",
    "    \"英\",\n",
    "    \"乌\",\n",
    "    \"厄\",\n",
    "    \"安\",\n",
    "    \"尤\",\n",
    "    \"恩\",\n",
    "    \"奥\",\n",
    "    \"阿\",\n",
    "    \"伊\",\n",
    "    \"埃\",\n",
    "    \"欧\",\n",
    "    \"艾\",\n",
    "    \"亚\"\n",
    "]\n",
    "\n",
    "for word_en, word_data in data.items():\n",
    "    if word_data[\"word_zh\"][0] in confident_onset_zh_list:\n",
    "        continue\n",
    "\n",
    "    x = transform_data.vowel_qualities(word_data[\"onset_en_ipa\"])\n",
    "    x.append(len(onset)-x[0])\n",
    "\n",
    "    x.append(word_data[\"stresses_en\"][0])\n",
    "\n",
    "    x.append(word_data[\"stresses_en\"].index(1))\n",
    "\n",
    "    x.append(onset_frequencies[word_data[\"onset_en_ipa\"]])\n",
    "\n",
    "    for tone in [1,2,3,4]:\n",
    "        x.append(cond_probs[\"onset_tone_num\"][str(tone)].get(onset, False) or 0)\n",
    "\n",
    "    for tone in [1,2,3,4]:\n",
    "        x.append(cond_probs[\"onset_en_ipa\"][onset].get(str(tone), False) or 0)\n",
    "\n",
    "    for onset_zh in onset_zh_list:\n",
    "        x.append(cond_probs[\"onset_zh\"][onset_zh].get(str(onset), False) or 0)\n",
    "\n",
    "    for onset_zh in onset_zh_list:\n",
    "        x.append(cond_probs[\"onset_en_ipa-onset_zh\"][onset].get(str(onset_zh), False) or 0)\n",
    "\n",
    "    x.append(len(word_data[\"syllables_en_arpa\"]) >= 2)\n",
    "\n",
    "    # x.append(len(word_data[\"onset_en_ipa\"])/len(\"\".join(word_data[\"syllables_en_ipa\"])))\n",
    "\n",
    "    X.append(x)\n",
    "    y.append(str(word_data[\"onset_tone_num\"]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create, train, and test model\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", random_state=100, min_samples_split=2, min_samples_leaf=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "with open(\"./models_expanded/clf_onset_tone_num.pkl\", \"wb\") as clf_onset_tone_num_file:\n",
    "    pickle.dump(clf, clf_onset_tone_num_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, False, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '4', '1')\n",
      "([1, False, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 116, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '3', '4')\n",
      "8 incorrectly predicted tone 3 (guessed tone 4 w/ decision path [0 4 6 8])\n",
      "([1, False, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 116, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '3', '4')\n",
      "8 incorrectly predicted tone 3 (guessed tone 4 w/ decision path [0 4 6 8])\n",
      "([1, False, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, False], '4', '1')\n",
      "([2, False, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 3, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '3', '1')\n",
      "2 incorrectly predicted tone 3 (guessed tone 1 w/ decision path [0 1 2])\n",
      "([1, False, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 14, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '4', '1')\n",
      "([2, False, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 2, 1, 29, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '3', '4')\n",
      "5 incorrectly predicted tone 3 (guessed tone 4 w/ decision path [0 4 5])\n",
      "([1, False, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 3, 0.10854092526690391, 0, 0, 0.09646302250803858, 0, 0, 0, 0, 0.14935064935064934, 0.05172413793103448, 0.2230769230769231, 0, 0, 0.043478260869565216, 0, 0.14285714285714285, 0.01818181818181818, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25274725274725274, 0.06593406593406594, 0.6373626373626373, 0, 0, 0.01098901098901099, 0, 0.02197802197802198, 0.01098901098901099, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, True], '2', '1')\n",
      "2 incorrectly predicted tone 2 (guessed tone 1 w/ decision path [0 1 2])\n",
      "{'1': 4, '2': 1, '3': 4, '4': 6} {'1': 4, '2': 0, '3': 0, '4': 3}\n",
      "1 accuracy: 4/4 (100.0%)\n",
      "2 accuracy: 0/1 (0.0%)\n",
      "3 accuracy: 0/4 (0.0%)\n",
      "4 accuracy: 3/6 (50.0%)\n",
      "[[4 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 3]\n",
      " [3 0 0 3]]\n",
      "0.25423758993656337\n",
      "Feature Onset N(V) w/ importance 0.0\n",
      "Feature Onset Length w/ importance 0.0\n",
      "Feature close w/ importance 0.0\n",
      "Feature near-close w/ importance 0.0\n",
      "Feature close-mid w/ importance 0.08181818181818189\n",
      "Feature mid w/ importance 0.0\n",
      "Feature open-mid w/ importance 0.0\n",
      "Feature near-open w/ importance 0.0\n",
      "Feature open w/ importance 0.0\n",
      "Feature front w/ importance 0.0\n",
      "Feature central w/ importance 0.0\n",
      "Feature back w/ importance 0.0\n",
      "Feature Onset N(C) w/ importance 0.0\n",
      "Feature initial_stress_en w/ importance 0.0\n",
      "Feature first_stress_en w/ importance 0.0\n",
      "Feature F(Onset) w/ importance 0.9181818181818181\n",
      "Feature P(Onset|T=1) w/ importance 0.0\n",
      "Feature P(Onset|T=2) w/ importance 0.0\n",
      "Feature P(Onset|T=3) w/ importance 0.0\n",
      "Feature P(Onset|T=4) w/ importance 0.0\n",
      "Feature P(T=1|Onset) w/ importance 0.0\n",
      "Feature P(T=2|Onset) w/ importance 0.0\n",
      "Feature P(T=3|Onset) w/ importance 0.0\n",
      "Feature P(T=4|Onset) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=奥) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=艾) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=阿) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=以) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=伊) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=亚) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=埃) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=欧) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=安) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=爱) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=秋) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=天) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=雅) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=猎) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=杜) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=夏) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=象) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=恩) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=尤) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=厄) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=乌) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=英) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=大) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=城) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=昂) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=东) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=翁) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=边) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=市) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=易) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=因) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=法) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=鹰) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=认) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=铠) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=竞) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=押) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=冰) w/ importance 0.0\n",
      "Feature P(Onset|Hanzi=我) w/ importance 0.0\n",
      "Feature P(Hanzi=奥|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=艾|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=阿|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=以|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=伊|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=亚|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=埃|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=欧|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=安|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=爱|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=秋|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=天|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=雅|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=猎|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=杜|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=夏|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=象|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=恩|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=尤|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=厄|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=乌|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=英|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=大|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=城|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=昂|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=东|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=翁|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=边|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=市|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=易|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=因|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=法|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=鹰|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=认|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=铠|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=竞|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=押|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=冰|Onset) w/ importance 0.0\n",
      "Feature P(Hanzi=我|Onset) w/ importance 0.0\n",
      "Feature Bisyllabic w/ importance 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      1.00      0.62         4\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.47        15\n",
      "   macro avg       0.24      0.38      0.28        15\n",
      "weighted avg       0.32      0.47      0.36        15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import print_metrics\n",
    "importlib.reload(print_metrics)\n",
    "\n",
    "print_metrics.print_metrics(X_test, y_test, y_pred, X_labels, y_labels, clf=clf, print_probas=False, print_sandhi_effects=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
